---
title: "Note of data science training EP 5: Logistic Regression & Dummy Classifier ‚Äì Divide and Predict"
layout: post
description: We could know all 3 types of Linear regression.
date: 2020-02-27 00:00:00 +0200
categories: [data, data science]
tags: [ML types, Scikit-learn, logistic regression, dummy classifier, Python]
image:
  path: ../assets/img/features/bilal-o-ljXekphwr40-unsplash.jpg
  alt: Unsplash / Bilal O.
  caption: <a href="https://unsplash.com/photos/blue-and-pink-water-droplets-ljXekphwr40">Unsplash / Bilal O.</a>
---

[expand-series]

  1. [Note of data science training EP 1: Intro ‚Äì unboxing]({% post_url 2020-01-12-data-sci-1 %})
  1. [Note of data science training EP 2: Pandas & Matplotlib ‚Äì from a thousand mile above]({% post_url 2020-01-24-data-sci-2 %})
  1. [Note of data science training EP 3: Matplotlib & Seaborn ‚Äì Luxury visualization]({% post_url 2020-01-24-data-sci-3 %})
  1. [Note of data science training EP 4: Scikit-learn & Linear Regression ‚Äì Linear trending]({% post_url 2020-02-17-data-sci-4 %})
  1. Note of data science training EP 5: Logistic Regression & Dummy Classifier ‚Äì Divide and Predict
  1. [Note of data science training EP 6: Decision Tree ‚Äì At a point of distraction]({% post_url 2020-03-02-data-sci-6 %})
  1. [Note of data science training EP 7: Metrics ‚Äì It is qualified]({% post_url 2020-03-12-data-sci-7 %})
  1. [Note of data science training EP 8: Ensemble ‚Äì Avenger's ensemble]({% post_url 2020-04-15-data-sci-8 %})
  1. [Note of data science training EP 9: NetworkX ‚Äì Map of Marauder in real world]({% post_url 2020-05-14-data-sci-9 %})
  1. [Note of data science training EP 10: Cluster ‚Äì collecting and clustering]({% post_url 2020-06-08-data-sci-10 %})
  1. [Note of data science training EP 11: NLP & Spacy ‚Äì Languages are borderless]({% post_url 2020-07-07-data-sci-11 %})
  1. [Note of data science training EP 12: skimage ‚Äì Look out carefully]({% post_url 2020-07-27-data-sci-12 %})
  1. [Note of data science training EP 13: Regularization ‚Äì make it regular with Regularization]({% post_url 2020-09-03-data-sci-13 %})
  1. [Note of data science training EP 14 END ‚Äì Data scientists did their mistakes]({% post_url 2020-09-19-data-sci-14 %})

[/expand-series]

From EP 4, we could know all 3 types of Linear regression. Linear regression is just one of 4 Machine Learning (ML) algorithms.

---

## 4 Types of ML

![types of ML](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-05/ML-main-4.png)
*4 main types of Machine Learning Algorithms*

**Supervised Learning** is ML on labeled output and we use the label to train our models. For example, we have data of age and income. We want to predict income at any age as labeled output is income.

In other hand, **Unsupervised Learning** is working on unlabeled output that means we have no initial result data to train models. We will talk about this later.

And **Continuous** means quantitative output as **Discrete** means qualitative output.

As the figure above, there are main 4 ML algorithms:

- **Classification**  
  Discrete Supervised Learning. This is for predicting group in limited values.  
  For example, we want to predict which group of interests for a customer when we have customer data as ages, occupations, genders, and interests.
- **Regression**  
  Continuous Supervised Learning. This is to predict output as it can be any values.  
  The example is the previous episode.
- **Clustering**  
  Discrete Unsupervised Learning. It is to group our data. For example, we want to group customers into 8 types leading to the hit advertisements from customer data.
- **Dimensionality reduction**  
  Continuous Unsupervised Learning. This is working on data optimization.  
  One day we will have tons of data in thousands columns, and Dimensionality reduction helps us find the most important columns to deal with in terms of data processing.

---

And this time, I am proud to present‚Ä¶

## Logistic Regression

**Logistic Regression** predicts the **probability**. Despite its name, its algorithm is classifier as the result is a member of labeled output.

Now it‚Äôs the time.

We reuse Titanic data again. Assign `x` as "Pclass", "is_male" that is calculated from "Sex", ‚ÄúAge", ‚ÄúSibSp‚Äù, and ‚ÄúFare‚Äù.

![prep x](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-05/Screen-Shot-2020-02-29-at-22.42.02.png)

Assign `y` as "Survived". We are going to predict the survivability.

![prep y](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-05/Screen-Shot-2020-02-29-at-22.42.11.png)

Utilize the module `sklearn.linear_model.LogisticRegression()` and run `train_test_split()`.

![train test](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-05/Screen-Shot-2020-02-29-at-22.33.03.png)

We put `solver='lbfgs'` to avoid a warning.

And `fit()`. Just peeking `coef_` and find the second column, "is_male", has negatively impact on "Survived".

![fit](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-05/Screen-Shot-2020-02-29-at-22.42.21.png)

Once we get the model then go to test. We create a crew with "Pclass" is 30, male, no "SibSp", and paid 25 as "Fare".

Our model predicts he was dead ("Survived" is 0). Please mourn for him üò¢.

After run `.predict_proba()` and we realize his survivability is just 10%.

![predict](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-05/Screen-Shot-2020-02-29-at-22.42.27.png)

Ok. Let‚Äôs see the correctness of the model. We create a `DataFrame` to combine "Survived", predicts, and a correction flag as "is_correct".

Average correctness of our model is 83%. Quite great.

![is_correct](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-05/Screen-Shot-2020-02-29-at-22.52.50.png)

---

## Dummy Classifier

Besides, we shall meet Dummy Classifier. This one provides the "baseline" classification model. It means we can apply this as a standard to evaluate our model.

Dummy Classifier run the model with simple rules and we can define the rule by the parameter `strategy`. This time is "most_frequent" that is to predict base of frequent values of the training set.

![dummy](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-05/Screen-Shot-2020-02-29-at-22.55.25.png)

Now we check the baseline correctness as it is just 52.7%. Our Logistic Regression model above works.

---

Classifiers is useful for data categorizing works and I hope this blog can be great for you to understand ML overview.

Let‚Äôs see what‚Äôs next.

Bye~

---

## References

- [Supervised vs. Unsupervised Learning](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d)
- [DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)
