---
title: "Note of data science training EP 11: NLP & Spacy – Languages are borderless"
layout: post
author: bluebirz
description: Computers are capable to learn human languagues.
date: 2020-07-07
categories: [data, data science]
tags: [Scikit-learn, NLP, Spacy, random forest, linear regression, Python]
series:
  key: data-sci
  index: 11
comment: true
image:
  path: https://images.unsplash.com/photo-1580610447943-1bfbef5efe07?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D
  lqip: https://images.unsplash.com/photo-1580610447943-1bfbef5efe07?q=10&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D
  alt: Unsplash / Bilal O.
  caption: <a href="https://unsplash.com/photos/blue-and-pink-water-droplets-ljXekphwr40">Unsplash / Bilal O.</a>
---

{% include bbz_custom/expand_series.html key=page.series.key index=page.series.index %}

Computers are capable to learn human languagues.

---

## Natural Language Processing (NLP)

It is the methodology to translate human languages to datasets to analyse. For instances, "I Love You" can be translated as "positive", "romantic", and "sentimental".

One basic term is "Tokenization" that is splitting a set of text into groups of words. We understand what we listen to by combining all meanings of words, so is computer.

Python has many libraries for this task. One is Spacy.

---

## Spacy

This problem is from my final project. It is to predict rating from cartoons' names. The steps are to split the names and transform into numbers and use Random Forest estimator as a predictor.

Let's go.

---

## 1. Install

Find Spacy package [here](https://spacy.io/usage#installation).

---

## 2. Prepare a dataset

The dataset is from Kaggle via [this link](https://www.kaggle.com/CooperUnion/anime-recommendations-database?select=anime.csv).

![dataset](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.45.18.png)

---

## 3. import libraries and files

Import `Pandas` and `.read_csv()`.

![pd read csv](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.45.49.png)

---

## 4. import spacy

As the dataset is in English, we have to download Spacy model `"en_core_web_sm"` with `.load()` then we got a class object.

At this step, we can use that object to tokenize (word splitting) as the figure below.

![spacy load](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.46.03.png){: style="max-width:85%;margin:auto;" .apply-border}

We can display tokenized text with `.text` and their parts of speech with `.pos_`.

![tokenized](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-26-at-20.20.51.png){: style="max-width:60%;margin:auto;" .apply-border}

---

## 5. Custom tokenization

We don't want special characters but only letters and numbers, so we need to improve the tokenizer with the regular expression in this method.

```py
import re
def splitter(val, processor):
    pattern = r'[0-9a-zA-Z]+'
    return [r.group().lower() for r in re.finditer(pattern, processor(val).text)]
```

![setup splitter](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.46.22.png){: style="max-width:85%;margin:auto;" .apply-border}

`[0-9a-zA-Z]+` means to capture only number (0 – 9), lowercases (a – z), and uppercases (A – Z). Sign symbol means the captures are one letter or more.

---

## 6. Tokenize them all

OK, we now have to tokenize all names.

```py
pattern_splitter = [splitter(n, processor) for n in anime.name]
pattern_splitter
```

![call splitter](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.46.30.png){: style="max-width:85%;margin:auto;" .apply-border}

Then we add the tokenized value in a new column "name_token".

```py
anime.loc[:, 'name_token'] = pd.Series(pattern_splitter)
anime
```

![output](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.46.39.png)

---

## 7. Cleanse before use

As we require rating to predict, we have to remove non-value of rating here.

![clean](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.46.50.png)

---

## 8. Make train and test sets

From all 12,064 rows, we are going to separate them into train set and test set. We apart 70% to train set here.

![train test](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.47.27.png)

---

## 9. Vectorizer

Vectorizer in Scikit-learn is to transform words to matrix. It applies TF-IDF formula to calculate frequency of each word in the matrix.

![tf-idf](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/vectorizer.png)

First to create `TfidfVectorizer` object.

![create tf-idf vectorizer](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.47.13.png){: stryle="max-width:85%;margin:auto;" .apply-border}

Run `.fit_transform()` on train set to learn words and store in the matrix then run `.transform()` on test set.

![fit_transform](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.47.58.png){: style="max-width:85%;margin:auto;" .apply-border}

---

## 10. Random Forest

We now at the time to train it. Start with create a Regressor.

![random forest regressor](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.48.12.png){: style="max-width:80%;margin:auto;" .apply-border}

Assign "y" as the rating of train set.

![prep y](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.48.24.png){: style="max-width:75%;margin:auto;" .apply-border}

Finally, run `.fit()` with the matrix and "y". Now we got an estimator.

![fit](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.48.30.png){: style="max-width:90%;margin:auto;" .apply-border}

---

## 11. Scores of Random Forest

After that, we have to scoring the estimator. Here we have mse = 1.64 .

![metrics](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.48.41.png){: style="max-width:85%;margin:auto;" .apply-border}

Try to compare predicted and real rating.

![compare](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.48.49.png){: style="max-width:80%;margin:auto;" .apply-border}

Then plot a graph. It might prove that, there are less relationships between name and rating of cartoons. Anyway, it is ok for the prediction results.

![plot](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.49.58.png)

---

## 12. Interesting features

We can find feature rankings by `.feature_importances_` of Random Forest and feature values by `.get_feature_names()` of vectorizer.

Use them altogether to find which feature value causes the highest rating.

![important features](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.50.45.png)

This is a `DataFrame` of feature names and feature importances.

![feature importance](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.51.23.png)

---

## 13. Linear Regression version

We are curious how about Linear Regression. As a result, its mse = 2.98 that is higher than the Random Forest.

OK. this one is worse.

![linear regression](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-11/Screen-Shot-2020-06-23-at-22.52.09.png)

---

## NLP with Thai language

The teacher recommended [pythainlp](https://github.com/PyThaiNLP/pythainlp). This library can interpret Thai text in similar style as Spacy.

---

This blog is just an introduction. We can go further by learning Content Classification, Sentiment Analysis, etc.

See you next time, Bye.
