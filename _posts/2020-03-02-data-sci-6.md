---
title: "Note of data science training EP 6: Decision Tree – At a point of distraction"
layout: post
author: bluebirz
description: Decision tree is a classification model in the same group as Logistic regression but likely to create a box over a group of data.
date: 2020-03-02 00:00:00 +0200
categories: [data, data science]
tags: [Scikit-learn, decision tree, Graphviz, Python]
image:
  path: https://images.unsplash.com/photo-1580610447943-1bfbef5efe07?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D
  alt: Unsplash / Bilal O.
  caption: <a href="https://unsplash.com/photos/blue-and-pink-water-droplets-ljXekphwr40">Unsplash / Bilal O.</a>
---

[expand-series]

  1. [Note of data science training EP 1: Intro – unboxing]({% post_url 2020-01-12-data-sci-1 %})
  1. [Note of data science training EP 2: Pandas & Matplotlib – from a thousand mile above]({% post_url 2020-01-24-data-sci-2 %})
  1. [Note of data science training EP 3: Matplotlib & Seaborn – Luxury visualization]({% post_url 2020-01-24-data-sci-3 %})
  1. [Note of data science training EP 4: Scikit-learn & Linear Regression – Linear trending]({% post_url 2020-02-17-data-sci-4 %})
  1. [Note of data science training EP 5: Logistic Regression & Dummy Classifier – Divide and Predict]({% post_url 2020-02-27-data-sci-5 %})
  1. Note of data science training EP 6: Decision Tree – At a point of distraction
  1. [Note of data science training EP 7: Metrics – It is qualified]({% post_url 2020-03-12-data-sci-7 %})
  1. [Note of data science training EP 8: Ensemble – Avenger's ensemble]({% post_url 2020-04-15-data-sci-8 %})
  1. [Note of data science training EP 9: NetworkX – Map of Marauder in real world]({% post_url 2020-05-14-data-sci-9 %})
  1. [Note of data science training EP 10: Cluster – collecting and clustering]({% post_url 2020-06-08-data-sci-10 %})
  1. [Note of data science training EP 11: NLP & Spacy – Languages are borderless]({% post_url 2020-07-07-data-sci-11 %})
  1. [Note of data science training EP 12: skimage – Look out carefully]({% post_url 2020-07-27-data-sci-12 %})
  1. [Note of data science training EP 13: Regularization – make it regular with Regularization]({% post_url 2020-09-03-data-sci-13 %})
  1. [Note of data science training EP 14 END – Data scientists did their mistakes]({% post_url 2020-09-19-data-sci-14 %})

[/expand-series]

From EP 5, here we go for the another classification model. It is …

---

## Decision tree

Decision tree is a classification model in the same group as Logistic regression but it is likely to **create a box** over a group of data. The boxes can be resizing smaller to cover the group of data as much as possible.

While Logistic regression is to draw a line to separate data into two groups straightforwardly.

It is, when there are outliers, Decision tree would be struggling to cover them and output pretty different result. On the other hand, Logistic regression is able to deal against them in a better way.

---

## Let's prepare the data

Assign `x` as a DataFrame of "Pclass", "is_male", and "Age".

![prep x](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-07-at-01.07.23.png)

Then assign `y` as classified "Fare" through our function using `.map()`.

![prep y](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-07-at-00.55.32.png)

---

## Grow the tree

First, create `DecisionTreeClassifier` with two main parameters:

- `criterion`  
  we can choose one of two followings:
  - `'gini'` from "gini impurity".  
      This is to measure how far and often the random results are incorrectly labels. It is for reducing misclassification.
  - `'entropy'` is to measure how uncertainly the results are.  
      This is for exploratory analysis.
- `max_depth`  
  it defines a number of layer of the tree. This time we define 3.

![prep tree](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-07-at-00.55.47.png)

Then `.fit()` them.

![fit](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-07-at-00.55.57.png)

Eh, what are all labeled results? We can use `.classes_`.

![tree class](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-10-at-23.45.39.png)

Done.

---

## Show the tree

Use this to view our tree.

```py
sklearn.tree.export_graphviz()
```

![graphviz](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-07-at-00.56.08.png)

It's hard to understand. Now we run this to generate an intuitive diagram.

```py
ry:
    from StringIO import StringIO
except ImportError:
    from io import StringIO

import sklearn.tree
import IPython.display
import pydot
File_obj = StringIO()
sklearn.tree.export_graphviz(tree, out_file=File_obj)
Graph = pydot.graph_from_dot_data(File_obj.getvalue())

IPython.display.Image(Graph[0].create_png())
```

![graphviz image](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-07-at-00.57.04.png)

Ok. Let's find the case inside. Just try the most-left branch.

- If `x[0]` or "Pclass" is less than or equal 1.5 (actually Pclass is 1)
- And if `x[1]` or "is_male" is less than or equal 0.5 (actually 0 or female)
- And if `x[2]` or "Age" is less than 43.5
- Then we got [2, 11, 23, 26]
- Compare to the all labeled results, this person can be "Quite rich" with 26 points.

|Cheap|Luxury|Medium|Quite rich|
|:-:|:-:|:-:|:-:|
|2|11|23|26|

We can find how effective each column is with .feature_importances_ where "feature" means the column we are interested in.

![feature importance](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-07-at-00.57.12.png)

Try predict one. Let's say this person is in "Pclass" 1, is female ("is_male" = False) and 40 years old. As a results, she was predicted as "Quite rich" in "Fare".

![predict](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-06/Screen-Shot-2020-03-07-at-00.57.19.png)

---

Here we go to the end and may have a question, **when we apply which** classification between Decision tree and Logistic regression.

As far as I known, if we **ensure the data is able to be 2 groups separately** by a straight line, we can use Logistic regression. Otherwise, use Decision tree.

Let's see next time.

Bye~

---

## References

- [Logistic Regression versus Decision Trees](https://blog.bigml.com/2016/09/28/logistic-regression-versus-decision-trees/)
- [ML \| Logistic Regression v/s Decision Tree Classification](https://www.geeksforgeeks.org/ml-logistic-regression-v-s-decision-tree-classification/)
- [Gini Impurity and Entropy](https://medium.com/@jason9389/gini-impurity-and-entropy-16116e754b27)
- [What is difference between Gini Impurity and Entropy in Decision Tree?](https://www.quora.com/What-is-difference-between-Gini-Impurity-and-Entropy-in-Decision-Tree)
- [Logistic Regression vs. Decision Tree](https://dzone.com/articles/logistic-regression-vs-decision-tree)
- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
