---
title: "Note of data science training EP 7: Metrics ‚Äì It is qualified"
layout: post
description: We need to validate how strong our models are.
date: 2020-03-12 00:00:00 +0200
categories: [data, data science]
tags: [Scikit-learn, bias, variance, overfitting, underfitting, metrics, GridSearchCV, Python]
math: true
image:
  path: ../assets/img/features/bilal-o-ljXekphwr40-unsplash.jpg
  alt: Unsplash / Bilal O.
  caption: <a href="https://unsplash.com/photos/blue-and-pink-water-droplets-ljXekphwr40">Unsplash / Bilal O.</a>
---

[expand-series]

  1. [Note of data science training EP 1: Intro ‚Äì unboxing]({% post_url 2020-01-12-data-sci-1 %})
  1. [Note of data science training EP 2: Pandas & Matplotlib ‚Äì from a thousand mile above]({% post_url 2020-01-24-data-sci-2 %})
  1. [Note of data science training EP 3: Matplotlib & Seaborn ‚Äì Luxury visualization]({% post_url 2020-01-24-data-sci-3 %})
  1. [Note of data science training EP 4: Scikit-learn & Linear Regression ‚Äì Linear trending]({% post_url 2020-02-17-data-sci-4 %})
  1. [Note of data science training EP 5: Logistic Regression & Dummy Classifier ‚Äì Divide and Predict]({% post_url 2020-02-27-data-sci-5 %})
  1. [Note of data science training EP 6: Decision Tree ‚Äì At a point of distraction]({% post_url 2020-03-02-data-sci-6 %})
  1. Note of data science training EP 7: Metrics ‚Äì It is qualified
  1. [Note of data science training EP 8: Ensemble ‚Äì Avenger's ensemble]({% post_url 2020-04-15-data-sci-8 %})
  1. [Note of data science training EP 9: NetworkX ‚Äì Map of Marauder in real world]({% post_url 2020-05-14-data-sci-9 %})
  1. [Note of data science training EP 10: Cluster ‚Äì collecting and clustering]({% post_url 2020-06-08-data-sci-10 %})
  1. [Note of data science training EP 11: NLP & Spacy ‚Äì Languages are borderless]({% post_url 2020-07-07-data-sci-11 %})
  1. [Note of data science training EP 12: skimage ‚Äì Look out carefully]({% post_url 2020-07-27-data-sci-12 %})
  1. [Note of data science training EP 13: Regularization ‚Äì make it regular with Regularization]({% post_url 2020-09-03-data-sci-13 %})
  1. [Note of data science training EP 14 END ‚Äì Data scientists did their mistakes]({% post_url 2020-09-19-data-sci-14 %})

[/expand-series]

After we played Supervised ML algorithms, Regression and Classification. At this time, we need to validate how strong our models are.

---

## Technical terms

- **Bias**  
  Bias data goes away from **"normal"** values.  
  For example, we found number 100 from the employees‚Äô age list while there should be less than 50 years old.
- **Variance**  
  This is located out of **"trending"** values.  
  For example, we found OT data of a factory department which is between 0 ‚Äì 20 hours as it should be narrower such as 0 ‚Äì 5 hours. This causes our prediction be more difficult.
- **Overfitting**  
  It is an event our model try to capture all of the data.  
  As a result, our model will have low bias but high variance. Decision tree is an example of this case.
- **Underfitting**  
  This is opposite to overfitting case, it tries to capture trending data.  
  Our model will have high bias but low variance. Regressions are the examples.

![balance](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-07/1_9hPX9pAO3jqLrzt0IE3JzA.png)
*Source: [Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)*

---

## What are tools to test models

### Regressors‚Äô benchmarks

Here is a recap from [EP 4]({% post_url 2020-02-17-data-sci-4 %}):

- **$$r^2$$ score**  
  Comparing predictions to the real result. Higher is better and maximum at 1.  
  This indicates how much performance our models are compared to the base line (refer to dummy model in the last episode).
- **$$MedAE$$ or Median Absolute Error**  
  Median of the errors. Lower is better.
- **$$MAE$$ or Mean Absolute Error**  
  Average of errors. Lower is better.  
  This identifies how many outliers in data.
- **$$MSE$$ or Mean Square Error**  
  Average of errors power 2. The lower is the better.  
  It refers how many errors affecting our model not to be normal distribution.

![metrics for regressors](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-07/Screen-Shot-2020-03-15-at-22.04.38.png)

### Classifiers‚Äô benchmarks

- **Accuracy score**  
  Ratio of correct predictions and number of predictions.  
  Worst at 0 and best at 1.
- **Precision score**  
  Ratio of correct positive prediction and number of positive predictions.  
  Worst at 0 and best at 1.
- **Recall score**  
  Ratio of correct positive prediction and number of positive real data.  
  Worst at 0 and best at 1.
- **$$F_1$$ score**  
  Calculated by the formula $$F_1 = (\frac{Precision^{-1} + Recall^{-1}}{2})^{-1} = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$$.  
  This will be used in most of real-world problems as this is robust to large number of negative real data.  
  Worst at 0 and best at 1.
- **Confusion matrix**  
  This matrix enumerates a number of each prediction and real data. Alternative way is `pd.crosstab()` which displays them as percentages.

![metrics for classifier](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-07/Screen-Shot-2020-03-15-at-21.18.40.png)

### Model Selection

How can we build a model that generates best scores?

Here is a solution. It‚Äôs a module `sklearn.model_selection.GridSearchCV()`.

For the example, we defined a `GridSearchCV()` with a parameter set as "criterion" is ‚Äúgini‚Äù and ‚Äúmax_depth‚Äù starts at 3 and less than 10. ‚Äúcv‚Äù is a default value for cross-validation algorithm (5 for newer version of the library at this writing time).

![GridSearchCV](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-07/Screen-Shot-2020-03-15-at-21.48.41.png)

As a result, the sample model can be the best when we define "max_depth" as 3 as shown in `.best_params` and the `.best_score` can be that high at 0.76.

---

These are basic model evaluations and I found lots of way while researching about it. You can try new methods to test your models and feel free to share me. üòÑ

See ya next time.

Bye~

---

## References

<!-- - <https://medium.com/@george.drakos62/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0> -->
<!-- - <https://peltarion.com/knowledge-center/documentation/evaluation-view/regression-loss-metrics> -->
- [Metrics ep.1 (Thai)](https://www.bualabs.com/archives/1968/what-is-confusion-matrix-what-is-metrics-accuracy-precision-recall-f1-score-difference-metrics-ep-1/)
- [Accuracy vs. F1-Score](https://medium.com/analytics-vidhya/accuracy-vs-f1-score-6258237beca2)
