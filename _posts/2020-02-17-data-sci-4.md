---
title: "Note of data science training EP 4: Scikit-learn & Linear Regression – Linear trending"
layout: post
description: Linear regression calculation that draws a straight line.
date: 2020-02-27 00:00:00 +0200
categories: [data, data science]
tags: [Theil-Sen, RANSAC, Huber, Scikit-learn, metrics, Python]
math: true
image:
  path: ../assets/img/features/bilal-o-ljXekphwr40-unsplash.jpg
  alt: Unsplash / Bilal O.
  caption: <a href="https://unsplash.com/photos/blue-and-pink-water-droplets-ljXekphwr40">Unsplash / Bilal O.</a>
---

[expand-series]

  1. [Note of data science training EP 1: Intro – unboxing]({% post_url 2020-01-12-data-sci-1 %})
  1. [Note of data science training EP 2: Pandas & Matplotlib – from a thousand mile above]({% post_url 2020-01-24-data-sci-2 %})
  1. [Note of data science training EP 3: Matplotlib & Seaborn – Luxury visualization]({% post_url 2020-01-24-data-sci-3 %})
  1. Note of data science training EP 4: Scikit-learn & Linear Regression – Linear trending
  1. [Note of data science training EP 5: Logistic Regression & Dummy Classifier – Divide and Predict]({% post_url 2020-02-27-data-sci-5 %})
  1. [Note of data science training EP 6: Decision Tree – At a point of distraction]({% post_url 2020-03-02-data-sci-6 %})
  1. [Note of data science training EP 7: Metrics – It is qualified]({% post_url 2020-03-12-data-sci-7 %})
  1. [Note of data science training EP 8: Ensemble – Avenger's ensemble]({% post_url 2020-04-15-data-sci-8 %})
  1. [Note of data science training EP 9: NetworkX – Map of Marauder in real world]({% post_url 2020-05-14-data-sci-9 %})
  1. [Note of data science training EP 10: Cluster – collecting and clustering]({% post_url 2020-06-08-data-sci-10 %})
  1. [Note of data science training EP 11: NLP & Spacy – Languages are borderless]({% post_url 2020-07-07-data-sci-11 %})
  1. [Note of data science training EP 12: skimage – Look out carefully]({% post_url 2020-07-27-data-sci-12 %})
  1. [Note of data science training EP 13: Regularization – make it regular with Regularization]({% post_url 2020-09-03-data-sci-13 %})
  1. [Note of data science training EP 14 END – Data scientists did their mistakes]({% post_url 2020-09-19-data-sci-14 %})

[/expand-series]

In EP 3, we now understand how to create some graphs. This episode we are going to analyze data in serious way.

One of basic data science knowledge is linear regression calculation that draws a straight line. It is $$ Y = aX + b $$ which is the best when the line passes most dots in a plane or closest.

![linear regression](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-04/line01.png)

Figure above is not a very good line. We call the dots that are not in the line as "outliers". They are errors. When we find there are too many outliers, it can be either our line is not enough fit or so many errors that is unable to draw a line.

Here are sample ways to create a good line.

---

### Theil-Sen estimator

Theil-Sen estimator randoms pairs of dots and create line between them. In the end, **find the average value** of those lines. This benefit is speed but not good if there are too many outliers or it produces inaccurate results.

---

## RANSAC algorithm

**RANSAC** stands for **RA**ndom **SA**mple **C**onsensus. It is to find the best line which pass through maximum dots.

This algorithm is depending to slope as the formula is $$ slope=\frac{y_1-y_2}{x_1-x_2} $$. It means this is resisting to outliers in Y-axis but not to ones in X-axis.

RANSAC is slower than Theil-Sen.

---

## Huber regression

**Huber** uses $$\epsilon$$ (epsilon) which is greater than 1.0 and calculate over epsilon to find the linear formula.

Huber is faster than the first two.

---

That was just a lecture. Now we go to code in Jupyter.

## Scikit-learn

Introduce `sklearn` or scikit-learn library. This is a great tool for data analysis and prediction.

We shall `import sklearn.linear_model` that is a collection of linear regression models. And we do `import sklearn.model_selection` for data correction in this case.

We try titanic data on column "Pclass", "Age", and “Fare”.

![sk prep](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-04/Screen-Shot-2020-02-23-at-23.24.12.png)

Now we want to predict "Fare" from "Pclass" and “Age”. Therefore, we assign `x` as the latter two and `y` as “Fare”.

Run `sklearn.model_selection.train_test_split()` to split both `x` and `y` into two each that are training group and testing group with 10% size of testing group (`test_size` = 0.1)

![sk train](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-04/Screen-Shot-2020-02-23-at-23.24.20.png)

### Scikit-learn with Theil-Sen

This time we finished preparing data. Let’s go for **Theil-Sen** first.

We create `TheilSenRegressor` object, run `fit()` with training group of `x` and `y`, `predict()` with testing group of `x` and… Gotcha! we got the predicted result of Theil-Sen.

We can show the results as below:

- `coef_` is slope or $$a$$ from $$Y = aX + b$$
- `intercept_` is $$b$$

![sk Theil-Sen](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-04/Screen-Shot-2020-02-23-at-23.24.28.png)

The formula from Theil-Sen estimator is $$fare=-13.19\times Pclass - 0.04\times age + 51.49$$.

We use `y_train.values.ravel()` to fix data type issue.

### Scikit-Learn with RANSAC

Second, **RANSAC**. Create `RANSACRegressor()`.

![sk RANSAC](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-04/Screen-Shot-2020-02-23-at-23.24.34.png)

Repeat the method and now we got this formula $$fare=-10.86\times Pclass + 0.02\times age + 40.80$$.

### Scikit-Learn with Huber

Last one, **Huber** as `HuberRegressor()`.

![sk Huber](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-04/Screen-Shot-2020-02-23-at-23.24.43.png)

We got $$fare=-21.23\times Pclass - 0.25\times age + 79.99$$.

### Comparison

Got all three and time to plot. Give x-axis as the real value that is the testing group of `y` and y-axis is the predicted result.

![sk compare](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-04/Screen-Shot-2020-02-23-at-23.24.55.png)

---

## Metrics

scikit-learn provides `sklearn.metrics` for evaluating prediction. This time is these 3:

- `r2_score()`  
  $$r^2$$ is coefficient of determination. Higher is better.
- `median_absolute_error()`  
  $$MedAE$$ is the median of errors between prediction and actual. Lower is better.
- `mean_absolute_error()`  
  $$MAE$$ is the mean of errors between prediction and actual. Lower is better.

![sk metrics](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-04/Screen-Shot-2020-02-23-at-23.46.23.png)

---

This episode was suddenly attacking us with lots of mathematics stuff. LOL.

Let’s see what’s next.

See ya. Bye~

---

## References

- [Robust linear estimator fitting](https://scikit-learn.org/stable/auto_examples/linear_model/plot_robust_fit.html)
- [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
- [A column-vector y was passed when a 1d array was expected](https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected)
- [Coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)
- [Median absolute error](https://www.oreilly.com/library/view/machine-learning-for/9781786469878/9f44e711-deb6-42de-abbd-524832ad32cc.xhtml)
- [Mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error)
