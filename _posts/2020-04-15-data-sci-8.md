---
title: "Note of data science training EP 8: Ensemble – Avenger's ensemble"
layout: post
author: bluebirz
description: Ensemble can calculate the best results from combining many algorithms on different feature sets.
date: 2020-04-15 00:00:00 +0200
categories: [data, data science]
tags: [Scikit-learn, ensemble, bagging, random forest, GridSearchCV, Python]
math: true
image:
  path: https://images.unsplash.com/photo-1580610447943-1bfbef5efe07?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D
  alt: Unsplash / Bilal O.
  caption: <a href="https://unsplash.com/photos/blue-and-pink-water-droplets-ljXekphwr40">Unsplash / Bilal O.</a>
---

[expand-series]

  1. [Note of data science training EP 1: Intro – unboxing]({% post_url 2020-01-12-data-sci-1 %})
  1. [Note of data science training EP 2: Pandas & Matplotlib – from a thousand mile above]({% post_url 2020-01-24-data-sci-2 %})
  1. [Note of data science training EP 3: Matplotlib & Seaborn – Luxury visualization]({% post_url 2020-01-24-data-sci-3 %})
  1. [Note of data science training EP 4: Scikit-learn & Linear Regression – Linear trending]({% post_url 2020-02-17-data-sci-4 %})
  1. [Note of data science training EP 5: Logistic Regression & Dummy Classifier – Divide and Predict]({% post_url 2020-02-27-data-sci-5 %})
  1. [Note of data science training EP 6: Decision Tree – At a point of distraction]({% post_url 2020-03-02-data-sci-6 %})
  1. [Note of data science training EP 7: Metrics – It is qualified]({% post_url 2020-03-12-data-sci-7 %})
  1. Note of data science training EP 8: Ensemble – Avenger's ensemble
  1. [Note of data science training EP 9: NetworkX – Map of Marauder in real world]({% post_url 2020-05-14-data-sci-9 %})
  1. [Note of data science training EP 10: Cluster – collecting and clustering]({% post_url 2020-06-08-data-sci-10 %})
  1. [Note of data science training EP 11: NLP & Spacy – Languages are borderless]({% post_url 2020-07-07-data-sci-11 %})
  1. [Note of data science training EP 12: skimage – Look out carefully]({% post_url 2020-07-27-data-sci-12 %})
  1. [Note of data science training EP 13: Regularization – make it regular with Regularization]({% post_url 2020-09-03-data-sci-13 %})
  1. [Note of data science training EP 14 END – Data scientists did their mistakes]({% post_url 2020-09-19-data-sci-14 %})

[/expand-series]

We have learnt to predict something with one model, one set of selected features (or columns), and one set of parameters.

And in EP 7, we can predict it with one model and one set of selected features. Parameters are the results of model selection process.

What if we cannot select features, let's say there are too many features to pick up?

---

## Ensemble

Ensemble is a class embedded in scikit-learn package. It can calculate the best results from **combining many algorithms** on different feature sets.

For example, we have many data dimension of residents such as geo-location, size, land price, number of floors, referent web rating etc. and we need to predict a price of a house in downtown. In this case we are experiencing the tough problems against those many features and this is what the Ensemble is for.

This time is the sample Ensemble types: Bagging and Random Forest.

---

## Bagging

Bagging stands for Bootstrap Aggregating. It creates different estimators on random dataset **over all features**. Here are some main parameter of this.

- `base_estimator`  
  Specify estimator type, Decision tree by default.
- `n_estimators`  
  Number of different estimators, 10 by default.
- `max_samples`  
  Number of sample sets for training model

Then `.fit()` and `.predict()`.

![ensemble](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-21.51.22.png)

Firstly, `import sklearn.ensemble` and create `BaggingRegressor()` with a `DecisionTreeRegressor()` inside.

Apply `n_estimators` as 5 and `max_samples` as 25. After prediction we found its $$MedAE$$ is **65,667**.

![predict](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-21.51.41.png)

Now we created 3 more models with different values of `n_estimators` and `max_samples`. The first one is the best here.

![fit](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-21.56.47.png)

As the latest episode, we try run `GridSearchCV()` over it.

![GridSearchCV](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-23.36.49.png)

The best estimator after computing can create a model with $$MedAE$$ by only **63,443 points**.

This uses **16 features** out of 44 features from the source.

---

## Random Forest

Right now we go for Random Forest. Random Forest is different from Bagging at Random Forest **computes on some features**.

We can apply Random Forest estimator in the same way as Decision tree. Just put parameters and `.fit()` then `.predict()`.

![randow forest](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-23.24.48.png)

Oh, we made an estimator from Random Forest and it's better and Bagging's one.

This $$MedAE$$ is just **14,334** with **17 features** occupied.

---

I can say this one is quite complex for me and need more practice.

Let's see what's next and I gonna share to you all.

Bye~

---

## References

- [BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)
- [Ensemble Methods in Machine Learning: What are They and Why Use Them?](https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f)
- [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
- [What is the difference between bagging and random forest if only one explanatory variable is used?](https://stats.stackexchange.com/questions/264129/what-is-the-difference-between-bagging-and-random-forest-if-only-one-explanatory)
