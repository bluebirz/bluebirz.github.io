---
title: "Note of data science training EP 8: Ensemble â€“ Avenger's ensemble"
layout: post
author: bluebirz
description: Ensemble can calculate the best results from combining many algorithms on different feature sets.
date: 2020-04-15
categories: [data, data science]
tags: [Scikit-learn, ensemble, bagging, random forest, GridSearchCV, Python]
series:
  key: data-sci
  index: 8
math: true
comment: true
image:
  path: https://images.unsplash.com/photo-1580610447943-1bfbef5efe07?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D
  lqip: https://images.unsplash.com/photo-1580610447943-1bfbef5efe07?q=10&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D
  alt: Unsplash / Bilal O.
  caption: <a href="https://unsplash.com/photos/blue-and-pink-water-droplets-ljXekphwr40">Unsplash / Bilal O.</a>
---

{% include bbz_custom/expand_series.html key=page.series.key index=page.series.index %}

We have learnt to predict something with one model, one set of selected features (or columns), and one set of parameters.

And in [EP 7]({% post_url 2020-03-12-data-sci-7 %}), we can predict it with one model and one set of selected features. Parameters are the results of model selection process.

What if we cannot select features, let's say there are too many features to pick up?

---

## Ensemble

Ensemble is a class embedded in scikit-learn package. It can calculate the best results from **combining many algorithms** on different feature sets.

For example, we have many data dimension of residents such as geo-location, size, land price, number of floors, referent web rating etc. and we need to predict a price of a house in downtown. In this case we are experiencing the tough problems against those many features and this is what the Ensemble is for.

This time is the sample Ensemble types: Bagging and Random Forest.

---

## Bagging

Bagging stands for Bootstrap Aggregating. It creates different estimators on random dataset **over all features**. Here are some main parameter of this.

- `base_estimator`  
  Specify estimator type, Decision tree by default.
- `n_estimators`  
  Number of different estimators, 10 by default.
- `max_samples`  
  Number of sample sets for training model

Then `.fit()` and `.predict()`.

![ensemble](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-21.51.22.png)

Firstly, `import sklearn.ensemble` and create `BaggingRegressor()` with a `DecisionTreeRegressor()` inside.

Apply `n_estimators` as 5 and `max_samples` as 25. After prediction we found its $$MedAE$$ is **65,667**.

![predict](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-21.51.41.png)

Now we created 3 more models with different values of `n_estimators` and `max_samples`. The first one is the best here.

![fit](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-21.56.47.png)

As the latest episode, we try run `GridSearchCV()` over it.

![GridSearchCV](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-23.36.49.png)

The best estimator after computing can create a model with $$MedAE$$ by only **63,443 points**.

This uses **16 features** out of 44 features from the source.

---

## Random Forest

Right now we go for Random Forest. Random Forest is different from Bagging at Random Forest **computes on some features**.

We can apply Random Forest estimator in the same way as Decision tree. Just put parameters and `.fit()` then `.predict()`.

![randow forest](https://bluebirzdotnet.s3.ap-southeast-1.amazonaws.com/note-data-science-eps/ep-08/Screen-Shot-2020-04-13-at-23.24.48.png)

Oh, we made an estimator from Random Forest and it's better and Bagging's one.

This $$MedAE$$ is just **14,334** with **17 features** occupied.

---

I can say this one is quite complex for me and need more practice.

Let's see what's next and I gonna share to you all.

Bye~

---

## References

- [BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)
- [Ensemble Methods in Machine Learning: What are They and Why Use Them?](https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f)
- [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
- [What is the difference between bagging and random forest if only one explanatory variable is used?](https://stats.stackexchange.com/questions/264129/what-is-the-difference-between-bagging-and-random-forest-if-only-one-explanatory)
